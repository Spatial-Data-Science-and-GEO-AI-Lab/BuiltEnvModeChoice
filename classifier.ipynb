{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163017ef",
   "metadata": {
    "id": "163017ef"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import datetime\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn_gbmi import *\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, plot_confusion_matrix, make_scorer\n",
    "from sklearn.inspection import plot_partial_dependence\n",
    "from sklearn.inspection import partial_dependence\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45996455",
   "metadata": {
    "id": "45996455"
   },
   "source": [
    "# Importing the data\n",
    "\n",
    "The data is in a SPSS supported 'SAV' file. We will import it using pandas library's `read_spss` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c432a1",
   "metadata": {
    "id": "11c432a1"
   },
   "outputs": [],
   "source": [
    "# Importing SPSS file with pandas\n",
    "raw = pd.read_spss(\"DESTINATION.sav\")\n",
    "print(f\"Number of samples: {raw.shape[0]}\")\n",
    "print(f\"Number of features: {raw.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5417750e",
   "metadata": {
    "id": "5417750e"
   },
   "source": [
    "# Inspecting data\n",
    "\n",
    "## Feature types\n",
    "It is important to inspect the column names, data type, and non-null count. For this dataset, there are 45415 samples and 66 columns in the raw data. All columns are of `floar64` type except one. However, there are multiple columns which are of categorical type or are encoded with one-hot code system and have been put as dummy variable. `mode1` is the response variable which is the target of this analysis. `Consolidat` column is the trip type defined by `1, 3, 2, 7, 5, 6, and 4` values. 1 is the home-to-work trip based on which the dataset will be filtered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2109e4",
   "metadata": {
    "id": "0b2109e4"
   },
   "outputs": [],
   "source": [
    "raw.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8fbefc",
   "metadata": {
    "id": "da8fbefc"
   },
   "source": [
    "# Data Pre-processing\n",
    "\n",
    "Before starting analysis, the dataset needs to be pre-processed so that we can use it to build our classifier models. The steps of pre-processing would be:\n",
    "1. Filter the sample trips in order to keep only the home-to-work trips.\n",
    "2. Drop a number of columns keeping only the relevant features.\n",
    "3. Rename the columns for convenience.\n",
    "4. Add a `low_income` column as a dummy variable.\n",
    "5. Convert the distances from meters to km.\n",
    "6. Check for missing data.\n",
    "7. Check for anomalies and delete samples with anomalous data.\n",
    "8. Split the columns to numerical and categorical columns.\n",
    "9. Scale numeric features.\n",
    "10. Convert coded columns from float to integer.\n",
    "\n",
    "After the processing steps, the final dataset will be ready to be put through classifier models.\n",
    "\n",
    "## Steps 1-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365986b0",
   "metadata": {
    "id": "365986b0"
   },
   "outputs": [],
   "source": [
    "# Copying the raw data to keep the original\n",
    "data = raw.copy(deep = True)\n",
    "\n",
    "# Selecting 'Consolida = 1', meaning only Home-to-Work trip\n",
    "data = data.loc[data['Consolidat'] == 1]\n",
    "\n",
    "# Selecting unecessary and repetative columns to drop\n",
    "cols_to_drop = ['Member_Cod', 'Origin_TAZ', 'Destinatio', 'Purpose', 'Consolidat', 'Mode', 'RESIDENCE_taz',\n",
    "                'Age_class', 'Adolescenc', 'Young', 'Adult', 'Senior_Adu', 'Employment', 'Worker', 'Student',\n",
    "                'Housewife', 'Househol_iNCOME', 'Househol_m', 'Househol_c', 'BINNED_JHB', 'job_rich', 'BINED_HH_I',\n",
    "                'ACTIVE', 'PUBLIC', 'BINNED_DTT', 'medium_DTT', 'LONGER_DTT', 'BINNED_PTA', 'MPTA',\n",
    "                'HPTA', 'Ward_old', 'PTA_D', 'DTT_R_KM', 'DTT_D_KM', 'POPU_R_ACRE', 'POPU_D_ACRE', 'EMPLOY_D_ACRE',\n",
    "                'EMPLOY_R_ACRE',  'Access_fac', 'transit_ac', 'balanced', 'Personal_I', 'DISTANCE']\n",
    "\n",
    "# Renaming the columns\n",
    "cols = ['id', 'travel_time', 'household_size', 'bicycle_own', 'gender', 'age', 'driving_license',\n",
    "        'link_node_ratio', 'dist_tran_stop',  'diversity', 'job_hh_ratio', 'pop_density', 'emp_density', 'mode',\n",
    "        'middle_income', 'high_income', 'car_own', 'dest_link_node_ratio',\n",
    "        'dest_dist_tran_stop', 'dest_diversity', 'dest_job_hh_ratio', 'dest_pop_density', 'dest_emp_density']\n",
    "\n",
    "# Drop Columns\n",
    "data.drop(cols_to_drop, axis = 1, inplace = True)\n",
    "\n",
    "# Rename columns\n",
    "data.columns = cols\n",
    "\n",
    "# Adding 'low_income' category column\n",
    "data['low_income'] = data.apply(lambda x: 1 if x['middle_income'] == 0 and x['high_income'] == 0 else 0, axis=1)\n",
    "\n",
    "# Converting distance from meter to kilometer\n",
    "data.loc[:, ['dist_tran_stop', 'dest_dist_tran_stop']] = data.loc[:, ['dist_tran_stop', 'dest_dist_tran_stop']]/1000\n",
    "\n",
    "# Checking the data snapshot\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95899ba6",
   "metadata": {
    "id": "95899ba6"
   },
   "source": [
    "## Step 7: Checking anomalies\n",
    "### Identifying anomalies with visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72f0150",
   "metadata": {
    "id": "a72f0150"
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(data = data, x='dest_link_node_ratio', y = 'travel_time')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88dc30a",
   "metadata": {
    "id": "b88dc30a"
   },
   "source": [
    "### Dropping anomalous data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62d8e4f",
   "metadata": {
    "id": "e62d8e4f"
   },
   "outputs": [],
   "source": [
    "data = data[data.loc[:, 'dest_link_node_ratio'] > 0]\n",
    "\n",
    "# Converting id to string\n",
    "data.reset_index(inplace=True, drop=True)\n",
    "data.loc[:, 'id'] = data.index\n",
    "data.loc[:, 'id'] = data.loc[:, 'id'].astype('str')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cce2ac",
   "metadata": {
    "id": "70cce2ac"
   },
   "outputs": [],
   "source": [
    "print(f\"Number of samples: {data.shape[0]}\")\n",
    "print(f\"Number of features: {data.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cdf3d7",
   "metadata": {
    "id": "b3cdf3d7"
   },
   "source": [
    "## Step 8: Converting feature types\n",
    "### Splitting categorical and numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c52ac9f",
   "metadata": {
    "id": "9c52ac9f"
   },
   "outputs": [],
   "source": [
    "#####\n",
    "# Coded: 'gender','driving_license', 'low_income', 'middle_income', 'high_income', 'mode' > transform them to integer\n",
    "\n",
    "# Numerical Columns: 'travel_time', 'household_size', 'age', 'car_own', 'bicycle_own', 'link_node_ratio', 'dist_tran_stop',\n",
    "# 'diversity', 'job_hh_ratio', 'pop_density', 'emp_density',\n",
    "# 'dest_link_node_ratio', 'dest_dist_tran_stop', 'dest_diversity', 'dest_job_hh_ratio',\n",
    "# 'dest_pop_density', 'dest_emp_density'\n",
    "####\n",
    "num_cols=['travel_time', 'household_size', 'age', 'car_own', 'bicycle_own', 'link_node_ratio', 'dist_tran_stop',\n",
    "          'diversity', 'job_hh_ratio', 'pop_density', 'emp_density',\n",
    "          'dest_link_node_ratio', 'dest_dist_tran_stop', 'dest_diversity', 'dest_job_hh_ratio',\n",
    "          'dest_pop_density', 'dest_emp_density']\n",
    "\n",
    "coded = ['gender','driving_license', 'low_income', 'middle_income', 'high_income', 'mode']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b89560c",
   "metadata": {
    "id": "1b89560c"
   },
   "source": [
    "## Step 9: Scaling numeric features\n",
    "\n",
    "The numeric columns have different range. To build a classifier, we need to scale the values so that none of the features get preference from the classifier based on the high range of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3527ed2e",
   "metadata": {
    "id": "3527ed2e"
   },
   "outputs": [],
   "source": [
    "df = data.copy(deep = True)\n",
    "df[coded] = df[coded].astype('int32')\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(df.loc[:, num_cols])\n",
    "df.loc[:, num_cols] = scaler.transform(df.loc[:, num_cols])\n",
    "\n",
    "df = df[num_cols+coded]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae4a3fd",
   "metadata": {
    "id": "2ae4a3fd"
   },
   "source": [
    "# Preparing dataset for classifier\n",
    "\n",
    "The `mode` feature is our target variable, y. The rest of the features are the X or independent variables. The dataset will be divided into two parts - training and test set. This is known as holdout method, where a certain percentage of data is held out for testing the model performance. We divided the dataset into training and test set in a 70:30 ratio. Thus, 70% of the samples will be used to train the classifiers and the rest of it will be used to test the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf7232b",
   "metadata": {
    "id": "ddf7232b"
   },
   "outputs": [],
   "source": [
    "y = df[['mode']]\n",
    "X = df.drop(['mode'], axis = 1)\n",
    "rs = 4\n",
    "\n",
    "# Splitting dataset into training and test set.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=rs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507237d1",
   "metadata": {
    "id": "507237d1"
   },
   "source": [
    "## Handling unbalanced data\n",
    "\n",
    "The dataset is a multiclass framework and the classes are `Private, Transit, and Active`. However, the class distribution in the dataset is unbalanced as we have seen before. Meaning, there are a lot more samples for `Transit` and `Active` than `Private`. This issue will negative impact any classifier model built on this dataset. The during the training iterations, the model will learn the characteristics of the `Transit` and `Active` classes better than `Private` class which is not desirable. The goal is to build a model that can predict and give us insight on how the mode choice varies and which features influences the choice. To have an accurate representation, we need the classifier model to be able to train on a balanced dataset. So, we will resolve the issue through over-sampling the minority class. Meaning, we will randomly duplicate the `Private` class samples during training process. This is a well-known solution for resolving issues with unbalanced datasets. Another approach to this issue is to under-sample the dataset. Meaning we try to match the number of samples from each class to the class with lowest occurances. However, this approach reduces the total number of samples drastically and hampers the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0cbeec",
   "metadata": {
    "id": "7c0cbeec"
   },
   "outputs": [],
   "source": [
    "# oversampling the dataset\n",
    "smote = SMOTE(sampling_strategy='not majority')\n",
    "# fit and apply the transform\n",
    "X_over, y_over = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4302dbca",
   "metadata": {
    "id": "4302dbca"
   },
   "outputs": [],
   "source": [
    "# Random seed\n",
    "rs = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pD5CRFVE5gMz",
   "metadata": {
    "id": "pD5CRFVE5gMz"
   },
   "source": [
    "# Model Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de62d564",
   "metadata": {
    "id": "de62d564"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(clf, grid, data_x, data_y):\n",
    "    # Define the evaluation procedure\n",
    "    \n",
    "    # define the grid search procedure\n",
    "    grid_search = GridSearchCV(estimator=clf, param_grid=grid, cv=10, scoring='accuracy', verbose = 1,\n",
    "                               return_train_score=True)\n",
    "    \n",
    "    # execute the grid search\n",
    "    grid_result = grid_search.fit(data_x, data_y)\n",
    "    \n",
    "    # summarize the best score and configuration\n",
    "    print(\"Best: %.2f using parameters: %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "    \n",
    "    # Performance metric report\n",
    "    y_pred = grid_result.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Plotting confusion matrix\n",
    "    plt.style.use('classic')\n",
    "    fig = plt.figure(figsize=(16,8))\n",
    "    ax1 = fig.add_subplot(121)\n",
    "    plot_confusion_matrix(grid_result.best_estimator_, data_x, data_y, cmap=plt.cm.Blues, normalize='true', ax=ax1)\n",
    "    ax2 = fig.add_subplot(122)\n",
    "    plot_confusion_matrix(grid_result.best_estimator_, X_test, y_test, cmap=plt.cm.Blues, normalize='true', ax=ax2)\n",
    "    plt.show()\n",
    "    \n",
    "    return grid_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RP2ItotS5kBf",
   "metadata": {
    "id": "RP2ItotS5kBf"
   },
   "source": [
    "## Evaluation Grid Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74232bfc",
   "metadata": {
    "id": "74232bfc"
   },
   "outputs": [],
   "source": [
    "def grid_plot(grid_result):\n",
    "    train_result = grid_result.cv_results_['mean_train_score']\n",
    "    test_result = grid_result.cv_results_['mean_test_score']\n",
    "    \n",
    "    mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "    plt.figure(figsize=(8,3))\n",
    "    #plt.style.use('classic')\n",
    "    \n",
    "    plt.plot(range(1, len(train_result)+1), train_result, 'b', label='Training accuracy')\n",
    "    plt.plot(range(1,len(test_result)+1), test_result, 'r', label='Testing accuracy')\n",
    "    \n",
    "    plt.xlabel('Model iterations')\n",
    "    plt.xticks(range(1, len(train_result)+1, 2), rotation=75)\n",
    "    plt.ylabel('Score (accuracy)')\n",
    "    \n",
    "    plt.legend(loc='upper right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JhS_OX5_5ntS",
   "metadata": {
    "id": "JhS_OX5_5ntS"
   },
   "source": [
    "# Parameter grid-search and cross-validation\n",
    "\n",
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be42b836",
   "metadata": {
    "id": "be42b836"
   },
   "outputs": [],
   "source": [
    "clf_rf = RandomForestClassifier(bootstrap = True, n_jobs = -1, random_state = rs, class_weight = 'balanced')\n",
    "\n",
    "# Define grid search parameters\n",
    "grid = {'n_estimators': [100, 500, 1000]\n",
    "        , 'criterion': ['gini', 'entropy']\n",
    "        , 'max_depth': list(range(1,10,2))}\n",
    "\n",
    "# Evaluate the model\n",
    "grid_result_rf = evaluate_model(clf_rf, grid, X_over, np.ravel(y_over))\n",
    "\n",
    "grid_plot(grid_result_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AUElIwig5zDP",
   "metadata": {
    "id": "AUElIwig5zDP"
   },
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e54cd3b",
   "metadata": {
    "id": "7e54cd3b"
   },
   "outputs": [],
   "source": [
    "clf_gbdt = GradientBoostingClassifier(random_state=rs)\n",
    "\n",
    "# Define grid search parameters\n",
    "grid = {'n_estimators': [100, 500, 1000]\n",
    "        , 'learning_rate': [1e-5, 1e-2, 0.1]\n",
    "        , 'max_depth': [5, 10]\n",
    "        , 'max_features': ['sqrt', 'log2']}\n",
    "\n",
    "# Evaluate the model\n",
    "grid_result_gbdt = evaluate_model(clf_gbdt, grid, X_over, np.ravel(y_over))\n",
    "\n",
    "grid_plot(grid_result_gbdt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "G64BODkl505K",
   "metadata": {
    "id": "G64BODkl505K"
   },
   "source": [
    "## Selecting Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "E5gjfCH6Bgg7",
   "metadata": {
    "id": "E5gjfCH6Bgg7"
   },
   "outputs": [],
   "source": [
    "## Best Model: Random Forest\n",
    "# rf_2 = grid_result_rf.best_estimator_\n",
    "clf_rf_best = RandomForestClassifier(criterion ='gini', max_depth = 9, n_estimators = 500, bootstrap = True, n_jobs = -1, random_state = rs, class_weight = 'balanced')\n",
    "# clf_rf_best = RandomForestClassifier(bootstrap = True, n_jobs = -1, random_state = rs, class_weight = 'balanced')\n",
    "clf_rf_best.fit(X_over, np.array(y_train).ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EJt9xHOD548V",
   "metadata": {
    "id": "EJt9xHOD548V"
   },
   "source": [
    "# Partial Dependance Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a7084f",
   "metadata": {
    "id": "65a7084f"
   },
   "outputs": [],
   "source": [
    "def plot_clf_pdp(data_x, feat_list, color, classf, target):\n",
    "  features = feat_list\n",
    "  labels = ['Travel time', 'Household Size', 'Age', 'No. of Car Owned', 'No. of Bicycle Owned',\n",
    "            'Link node ratio', 'Distance to Transit Stop (km)', 'Diversity', 'Job-to-HH ratio (jobs/HH)',\n",
    "            'Population Density (HH/hectare)', 'Employment Density (jobs/hectare)','Destination Link node ratio',\n",
    "            'Destination Distance to Transit Stop (km)',\n",
    "            'Destination Diversity', 'Destination Job-to-HH ratio (jobs/hh)',\n",
    "            'Destination Population Density (HH/hectare)', 'Destination Employment Density (jobs/hectare)',\n",
    "            'gender', 'driving_license', 'income']\n",
    "\n",
    "  mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "  fig, ax = plt.subplots(figsize=(15,10))\n",
    "  disp = plot_partial_dependence(classf, data_x, features, target = target, feature_names=labels,\n",
    "                                        response_method = 'predict_proba',\n",
    "                                        ax=ax, line_kw={\"color\": color})\n",
    "  disp.axes_[0,0].grid(True, color='#dfe4ea')\n",
    "  disp.axes_[0,1].grid(True, color='#dfe4ea')\n",
    "  disp.axes_[0,2].grid(True, color='#dfe4ea')\n",
    "  disp.axes_[1,0].grid(True, color='#dfe4ea')\n",
    "  disp.axes_[1,1].grid(True, color='#dfe4ea')\n",
    "  disp.axes_[1,2].grid(True, color='#dfe4ea')\n",
    "\n",
    "  plt.subplots_adjust(top=0.9)\n",
    "  plt.show()\n",
    "\n",
    "  return disp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zoCh75qY58W4",
   "metadata": {
    "id": "zoCh75qY58W4"
   },
   "source": [
    "## Origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5887b03",
   "metadata": {
    "id": "b5887b03"
   },
   "outputs": [],
   "source": [
    "plot_clf_pdp(X_over, [5,6,7,8,9,10], \"#FC427B\", clf_rf_best, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YUFfjTTM596O",
   "metadata": {
    "id": "YUFfjTTM596O"
   },
   "source": [
    "## Destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68ef381",
   "metadata": {
    "id": "f68ef381"
   },
   "outputs": [],
   "source": [
    "plot_clf_pdp(X_over, [11,12,13,14,15,16], \"#FC427B\", clf_rf_best, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gDpdq3k15_nA",
   "metadata": {
    "id": "gDpdq3k15_nA"
   },
   "source": [
    "# Dual Partial Dependance Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ABBU_LZc4woW",
   "metadata": {
    "id": "ABBU_LZc4woW"
   },
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "features =[(9,7),(9,10),(6,8), (15,11),(15,16),(14,16)]\n",
    "partial = []\n",
    "\n",
    "for feats in features:\n",
    "  p = partial_dependence(clf_rf_best, X_over, features=feats, kind='average', grid_resolution=20)\n",
    "  partial.append(p)\n",
    "\n",
    "labels = ['Travel time', 'Household Size', 'Age', 'No. of Car Owned', 'No. of Bicycle Owned',\n",
    "          'Link node ratio', 'Distance to Transit Stop (km)', 'Diversity', 'Job-to-HH ratio (jobs/HH)',\n",
    "          'Population Density (HH/hectare)', 'Employment Density (jobs/hectare)','Destination Link node ratio',\n",
    "          'Destination Distance to Transit Stop (km)',\n",
    "          'Destination Diversity', 'Destination Job-to-HH ratio (jobs/hh)',\n",
    "          'Destination Population Density (HH/hectare)', 'Destination Employment Density (jobs/hectare)',\n",
    "          'gender', 'driving_license', 'income']\n",
    "\n",
    "fig = plt.figure(figsize=(18,12))\n",
    "\n",
    "for i, p in enumerate(partial):\n",
    "  ax = fig.add_subplot(2,3,i+1)\n",
    "  XX, YY = np.meshgrid(p[\"values\"][0], p[\"values\"][1])\n",
    "  Z = p.average[0].T\n",
    "  surf = ax.contourf(XX, YY, Z)\n",
    "  ax.set_xlabel(labels[features[i][0]], fontsize = 15)\n",
    "  ax.set_ylabel(labels[features[i][1]], fontsize = 15)\n",
    "\n",
    "  cbar = fig.colorbar(surf, format='%.3f')\n",
    "  cbar.ax.set_ylabel('Car choice probability', fontsize = 15)\n",
    "\n",
    "plt.subplots_adjust(top=0.9)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "tot = datetime.now()-now\n",
    "print(f\"Total time to plot: {tot.seconds/60:.2f} miunutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "djXETHQA6CFT",
   "metadata": {
    "id": "djXETHQA6CFT"
   },
   "source": [
    "# H-Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s22JV0gfCGdR",
   "metadata": {
    "id": "s22JV0gfCGdR"
   },
   "outputs": [],
   "source": [
    "pair_h = h_all_pairs(clf_rf_best, X_over)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "envml",
   "language": "python",
   "name": "envml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
